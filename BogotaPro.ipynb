{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGdpjC3d97wy"
      },
      "source": [
        "Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsZajgVmDAFP"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eljp2x4EDkqG"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X46LPokHUc3"
      },
      "outputs": [],
      "source": [
        "!pip install xlsxwriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8ao-VxI2k5K",
        "outputId": "2b30880f-983d-4c70-d5fb-ae5fd57fef5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oVNZMYf0CSs"
      },
      "source": [
        "# Preparation\n",
        "- Create a function that create excel to hold the results of the models (Will be used later).\n",
        "- Extract the relevant data (the client sentences) from the dataset and hold it in a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGUwmtvbF77V"
      },
      "outputs": [],
      "source": [
        "import pandas\n",
        " \n",
        "def creating_excel() -> pandas.DataFrame:\n",
        "      # Create an Excel file\n",
        "      writer = pandas.ExcelWriter('Results.xlsx', engine='xlsxwriter')\n",
        "\n",
        "      # Set the column names\n",
        "      data = {'Sentence': [],'Category': [], 'Nouns': [] }\n",
        "\n",
        "      # Convert the dataframe to an XlsxWriter Excel object.\n",
        "      output_excel = pandas.DataFrame(data)\n",
        "      output_excel.to_excel(writer, sheet_name='Sheet1', index=False)\n",
        "      print('Excel Created')\n",
        "      return output_excel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-45XRDMAKmyr"
      },
      "outputs": [],
      "source": [
        "import pandas\n",
        "from xlsxwriter import Workbook\n",
        "\n",
        "input_excel = pandas.read_excel('/content/drive/MyDrive/src/Chatbot dataset.xlsx') # data set\n",
        "\n",
        "client_message = [] # 377 sentence by client\n",
        "\n",
        "# rows num\n",
        "n_rows = len(input_excel.index)\n",
        "\n",
        "# columns num\n",
        "n_cols_ = len(input_excel.columns)\n",
        "\n",
        "for row in range(n_rows):\n",
        "  username = input_excel.iloc[row][2]\n",
        "  if username == 'client':\n",
        "    message = input_excel.iloc[row][3]\n",
        "    client_message.append(message)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGFkFeK79dyK"
      },
      "source": [
        "# Unsupervised  classification\n",
        "- First step: We use Named entity recognition with Bert module to get the nouns in the sentence.\n",
        "- Second Step: We use smaller-LaBSE(Language-agnostic BERT Sentence Embedding) model to get the sentences embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4151_4JyDW3W"
      },
      "source": [
        "# First step\n",
        "We use Named entity recognition with Bert module to get the nouns in the sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sV-OsoJT9_D_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# Load the model and the tokenizer from the downloaded files.\n",
        "# I added 'map_location=torch.device('cpu')' bcz I use only cpu\n",
        "model = torch.load(r\"/content/drive/MyDrive/src/my_model_3.pth\", map_location=torch.device('cpu'))\n",
        "tokenizer = torch.load(r\"/content/drive/MyDrive/src/my_tokenizer.pth\", map_location=torch.device('cpu'))\n",
        "\n",
        "all_sentence_nouns = []\n",
        "\n",
        "# Our input\n",
        "for sentence in client_message:\n",
        "  tokenized_sentence = tokenizer.encode(sentence)  # list of numbers represent each word\n",
        "  input_ids = torch.tensor([tokenized_sentence])  # I removed the .cuda() bcz I used only cpu on my computer\n",
        "\n",
        "\n",
        "  tag_values = ['DT', 'POS', 'NNS', 'VBG', 'CD', ';', 'JJS', 'NN', 'RP', '.', 'WP', 'PRP', 'CC', 'WRB', 'RBR', 'MD', 'VBZ', 'UH', 'FW', 'PDT',\n",
        "                'NNP', ':', 'JJ', 'JJR', 'RRB', '$', 'VB', ',', 'VBP', 'PRP$', 'NNPS', '``', 'IN', 'EX', 'TO', 'RB', 'VBN', 'RBS', 'WDT', 'LRB', 'VBD', 'WP$', 'PAD']\n",
        "\n",
        "  with torch.no_grad():\n",
        "      output = model(input_ids)\n",
        "\n",
        "  label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
        "\n",
        "  # join bpe split tokens\n",
        "  tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
        "\n",
        "  new_tokens, new_labels, nouns_from_sentence = [] ,[], []\n",
        "\n",
        "  for token, label_idx in zip(tokens, label_indices[0]):\n",
        "      if token.startswith(\"##\"):\n",
        "          new_tokens[-1] = new_tokens[-1] + token[2:]\n",
        "      else:\n",
        "          new_labels.append(tag_values[label_idx])\n",
        "          new_tokens.append(token)\n",
        "\n",
        "  for token, label in zip(new_tokens, new_labels):\n",
        "      if 'NN' in label and '[SEP]' not in token and '[CLS]' not in token and '?' not in token:\n",
        "          nouns_from_sentence.append(token)\n",
        "\n",
        "  all_sentence_nouns.append(nouns_from_sentence) # [['noun1','noun2',..],[]]\n",
        "\n",
        "print(f'all_sentence_nouns = {all_sentence_nouns}')\n",
        "print(f'client_message = {client_message}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED4G8L6Bpyvl"
      },
      "source": [
        "# Second step\n",
        "We use smaller-LaBSE(Language-agnostic BERT Sentence Embedding) model to get the sentences embeddings.\n",
        "We have 3 options:\n",
        "1. Get the vector for *all* the sentence.\n",
        "2. Get the vector for a *concatenation string of the nouns* in the sentence.\n",
        "3. Get a vector *for each noun in the sentence* and sum the values for each category you get and then calc the avg of all the nouns in the sentence and return the max.\n",
        "\n",
        "For each option we check what category the sentence belong to (by calc the arithmetic distance between the vector that represent the sentence and the vectors that represent the categories).\n",
        "\n",
        "# The first option\n",
        "Get the vector for *all* the sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOyZfCFpDcC0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text  # noqa\n",
        "import tensorflow_hub as hub\n",
        "from xlsxwriter import Workbook\n",
        "\n",
        "TRESHOLD = 0.22\n",
        "\n",
        "# Loading models from tfhub.dev\n",
        "encoder = hub.KerasLayer(\"https://tfhub.dev/jeongukjae/smaller_LaBSE_15lang/1\")\n",
        "preprocessor = hub.KerasLayer(\"https://tfhub.dev/jeongukjae/smaller_LaBSE_15lang_preprocess/1\")\n",
        "\n",
        "# Constructing model to encode texts into high-dimensional vectors\n",
        "sentences = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"sentences\")\n",
        "encoder_inputs = preprocessor(sentences)\n",
        "sentence_representation = encoder(encoder_inputs)[\"pooled_output\"]\n",
        "normalized_sentence_representation = tf.nn.l2_normalize(sentence_representation, axis=-1)  # for cosine similarity\n",
        "model = tf.keras.Model(sentences, normalized_sentence_representation)\n",
        "\n",
        "# Start Algo\n",
        "\n",
        "index_category = {0:'Environment and climate resilience',1:'Mobility (transport)',2:'Local identity',3:'Future of work',4:'Land use'}\n",
        "\n",
        "output_excel = creating_excel()  # create an Excel file\n",
        "excel_index = 0\n",
        "\n",
        "temp = 0 # for break\n",
        "for sentence in client_message:\n",
        "  # Encoding the messages and the categories sentences.\n",
        "  messages_sentences = tf.constant([sentence])\n",
        "  categories_sentences = tf.constant([\"Environment and climate resilience\", \"Mobility (transport)\", \"Local identity\", \"Future of work\", \"Land use\"])\n",
        "\n",
        "  messages_embeds = model(messages_sentences)\n",
        "  categories_embeds = model(categories_sentences)\n",
        "\n",
        "  # Messages-categories similarity\n",
        "  result = tf.tensordot(messages_embeds, categories_embeds, axes=[[1], [1]])\n",
        "\n",
        "  # write the sentence in the excel\n",
        "  output_excel.at[excel_index, 'Sentence'] = sentence\n",
        "\n",
        "  for value in result: # result = [[3432 34234 234 324234 23]]\n",
        "    for i,v in enumerate(value): # for each number in the list\n",
        "      if float(v) > TRESHOLD: # needs to be change accorindg to the result from ChatGPT\n",
        "        output_excel.at[excel_index, 'Category'] = index_category.get(i) + ','\n",
        "\n",
        "  excel_index += 1\n",
        "  if temp > 100:\n",
        "    break\n",
        "  temp += 1\n",
        "\n",
        "output_excel.to_excel(\"Results.xlsx\", index=False)  # save the Excel file\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWFDEEvUoHaB"
      },
      "source": [
        "# The second option\n",
        "Get the vector for a *concatenation string of the nouns* in the sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81Q-7zojoGJq",
        "outputId": "ee005856-f5a4-4207-b232-2acf0cd120e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Excel Created\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text  # noqa\n",
        "import tensorflow_hub as hub\n",
        "from xlsxwriter import Workbook\n",
        "\n",
        "TRESHOLD = 0.3\n",
        "\n",
        "# Loading models from tfhub.dev\n",
        "encoder = hub.KerasLayer(\"https://tfhub.dev/jeongukjae/smaller_LaBSE_15lang/1\")\n",
        "preprocessor = hub.KerasLayer(\"https://tfhub.dev/jeongukjae/smaller_LaBSE_15lang_preprocess/1\")\n",
        "\n",
        "# Constructing model to encode texts into high-dimensional vectors\n",
        "sentences = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"sentences\")\n",
        "encoder_inputs = preprocessor(sentences)\n",
        "sentence_representation = encoder(encoder_inputs)[\"pooled_output\"]\n",
        "normalized_sentence_representation = tf.nn.l2_normalize(sentence_representation, axis=-1)  # for cosine similarity\n",
        "model = tf.keras.Model(sentences, normalized_sentence_representation)\n",
        "\n",
        "# Start Algo\n",
        "index_category = {0:'Environment and climate resilience',1:'Mobility (transport)',2:'Local identity',3:'Future of work',4:'Land use'}\n",
        "\n",
        "output_excel = creating_excel()  # create an Excel file\n",
        "excel_index = 0\n",
        "\n",
        "# temp = 0 # for break\n",
        "for nouns,sentence in zip(all_sentence_nouns,client_message):\n",
        "  \n",
        "  # print(f'nouns = {nouns}') # nouns = ['places', 'students', 'building']\n",
        "  # print(f'sentence = {sentence}') # \"I think there should be many various sitting..\"\n",
        "\n",
        "  # when list of nouns is empty, continue to the next iteration\n",
        "  if len(nouns) == 0:\n",
        "    continue\n",
        "\n",
        "  # creates a concatenated string of all nouns\n",
        "  conca_string = ' '.join(nouns)\n",
        "\n",
        "  # Encoding the messages and the categories sentences.\n",
        "  messages_sentences = tf.constant([conca_string])\n",
        "  categories_sentences = tf.constant([\"Environment and climate resilience\", \"Mobility (transport)\", \"Local identity\", \"Future of work\", \"Land use\"])\n",
        "\n",
        "  messages_embeds = model(messages_sentences)\n",
        "  categories_embeds = model(categories_sentences)\n",
        "\n",
        "  # Messages-categories similarity\n",
        "  result = tf.tensordot(messages_embeds, categories_embeds, axes=[[1], [1]])\n",
        "\n",
        "  # write the sentence in the excel\n",
        "  output_excel.at[excel_index, 'Sentence'] = sentence\n",
        "  output_excel.at[excel_index, 'Nouns'] = conca_string\n",
        "\n",
        "  category = ''\n",
        "  for value in result: # result = [[3432 34234 234 324234 23]]\n",
        "    for i,v in enumerate(value): # for each number in the list\n",
        "      if float(v) > TRESHOLD: # needs to be change accorindg to the result from ChatGPT\n",
        "        category += index_category.get(i) + ','\n",
        "    output_excel.at[excel_index, 'Category'] = category\n",
        "\n",
        "  excel_index += 1\n",
        "  if temp > 100:\n",
        "    break\n",
        "  temp += 1\n",
        "\n",
        "output_excel.to_excel(\"Results.xlsx\", index=False)  # save the Excel file\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuE9qsiE5VU-"
      },
      "source": [
        "# The third option\n",
        "Get a vector *for each noun in the sentence* and sum the values for each category you get and then calc the avg of all the nouns in the sentence and return the max:\n",
        "\n",
        "**example:**\n",
        "\n",
        "  sentence = \"I think there should be many various sitting and studying    places for students, both inside and outside of the building.\"\n",
        "\n",
        "  nouns = places students building\n",
        "\n",
        "  call the model on each noun -> we get [value1,value2,...,value5]\n",
        "\n",
        "  sum all the values for each categoty and then return the category with the max value.\n",
        "\n",
        "  \n",
        "\n",
        "smaller-LaBSE.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afTMBJ980aqG"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text  # noqa\n",
        "import tensorflow_hub as hub\n",
        "#from xlsxwriter import Workbook\n",
        "\n",
        "TRESHOLD = 0.22\n",
        "\n",
        "# Loading models from tfhub.dev\n",
        "encoder = hub.KerasLayer(\"https://tfhub.dev/jeongukjae/smaller_LaBSE_15lang/1\")\n",
        "preprocessor = hub.KerasLayer(\"https://tfhub.dev/jeongukjae/smaller_LaBSE_15lang_preprocess/1\")\n",
        "\n",
        "# Constructing model to encode texts into high-dimensional vectors\n",
        "sentences = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"sentences\")\n",
        "encoder_inputs = preprocessor(sentences)\n",
        "sentence_representation = encoder(encoder_inputs)[\"pooled_output\"]\n",
        "normalized_sentence_representation = tf.nn.l2_normalize(sentence_representation, axis=-1)  # for cosine similarity\n",
        "model = tf.keras.Model(sentences, normalized_sentence_representation)\n",
        "\n",
        "# Start Algo\n",
        "index_category = {0:'Environment and climate resilience',1:'Mobility (transport)',2:'Local identity',3:'Future of work',4:'Land use'}\n",
        "\n",
        "output_excel = creating_excel()  # create an Excel file\n",
        "excel_index = 0\n",
        "sum_result_column = [0 for i in range(5)]\n",
        "\n",
        "temp = 0 # for break\n",
        "for nouns,sentence in zip(all_sentence_nouns,client_message):\n",
        "  \n",
        "  # print(f'nouns = {nouns}') # nouns = ['places', 'students', 'building']\n",
        "  # print(f'sentence = {sentence}') # \"I think there should be many various sitting..\"\n",
        "\n",
        "  # when list of nouns is empty\n",
        "  if len(nouns) == 0:\n",
        "    continue\n",
        "\n",
        "  # creates a concatenated string of all nouns\n",
        "  conca_string = ' '.join(nouns)\n",
        "\n",
        "  for noun in nouns:\n",
        "\n",
        "    # Encoding the messages and the categories sentences.\n",
        "    messages_sentences = tf.constant([noun])\n",
        "    categories_sentences = tf.constant([\"Environment and climate resilience\", \"Mobility (transport)\", \"Local identity\", \"Future of work\", \"Land use\"])\n",
        "\n",
        "    messages_embeds = model(messages_sentences)\n",
        "    categories_embeds = model(categories_sentences)\n",
        "\n",
        "    # Messages-categories similarity\n",
        "    result = tf.tensordot(messages_embeds, categories_embeds, axes=[[1], [1]])\n",
        "\n",
        "    # write the sentence in the excel\n",
        "    output_excel.at[excel_index, 'Sentence'] = sentence\n",
        "    output_excel.at[excel_index, 'Nouns'] = conca_string\n",
        "\n",
        "    for value in result: # result = [[3432 34234 234 324234 23]]\n",
        "      for i,v in enumerate(value): # for each number in the list\n",
        "        sum_result_column[i] += v\n",
        "\n",
        "  category = ''\n",
        "  for i,value in enumerate(sum_result_column):\n",
        "    if float(value) > TRESHOLD:\n",
        "      category += index_category.get(i) + ','\n",
        "  output_excel.at[excel_index, 'Category'] = category\n",
        "\n",
        "  print(f'temp = {temp}')\n",
        "  excel_index += 1\n",
        "  if temp > 60:\n",
        "    break\n",
        "  temp += 1\n",
        "\n",
        "output_excel.to_excel(\"Results.xlsx\", index=False)  # save the Excel file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9Cvsouu9GZS"
      },
      "source": [
        "# Supervised classification\n",
        "- Use ChatGPT API to classify the sentences to the right categories \n",
        "\n",
        "# Algorithm\n",
        "\n",
        "- Create a function that get a sentence as input and return a list of the nouns\n",
        "\n",
        "- Create a function that get a sentence as input and return the cos similarity between the sentence and the 5 categories \n",
        "\n",
        "- Create a function that get a sentence, nouns and cos similarity of the sentence and return the classification from Chat GPT for this sentence. Send a querry to ChatGPT with the sentence, the nouns of the sentence and the cos similarity of the sentence with the 5 categories.\n",
        "\n",
        "- Train a model using the classifier adaboost and the embedding TF IDF to get the best result. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "px0dAGsDL9bU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "def get_nouns(sentence):\n",
        "    # Load the model and the tokenizer from the downloaded files.\n",
        "    # I added 'map_location=torch.device('cpu')' bcz I use only cpu\n",
        "    model = torch.load(r\"/content/drive/MyDrive/src/my_model_3.pth\", map_location=torch.device('cpu'))\n",
        "    tokenizer = torch.load(r\"/content/drive/MyDrive/src/my_tokenizer.pth\", map_location=torch.device('cpu'))\n",
        "\n",
        "    # Our input\n",
        "    tokenized_sentence = tokenizer.encode(sentence)  # list of numbers represent each word\n",
        "    input_ids = torch.tensor([tokenized_sentence])  # I removed the .cuda() bcz I used only cpu on my computer\n",
        "\n",
        "\n",
        "    tag_values = ['DT', 'POS', 'NNS', 'VBG', 'CD', ';', 'JJS', 'NN', 'RP', '.', 'WP', 'PRP', 'CC', 'WRB', 'RBR', 'MD', 'VBZ', 'UH', 'FW', 'PDT',\n",
        "                'NNP', ':', 'JJ', 'JJR', 'RRB', '$', 'VB', ',', 'VBP', 'PRP$', 'NNPS', '``', 'IN', 'EX', 'TO', 'RB', 'VBN', 'RBS', 'WDT', 'LRB', 'VBD', 'WP$', 'PAD']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_ids)\n",
        "\n",
        "    label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
        "\n",
        "    # join bpe split tokens\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
        "\n",
        "    new_tokens, new_labels, nouns_from_sentence = [] ,[], []\n",
        "\n",
        "    for token, label_idx in zip(tokens, label_indices[0]):\n",
        "        if token.startswith(\"##\"):\n",
        "            new_tokens[-1] = new_tokens[-1] + token[2:]\n",
        "        else:\n",
        "            new_labels.append(tag_values[label_idx])\n",
        "            new_tokens.append(token)\n",
        "    ans = \"\"\n",
        "    for token, label in zip(new_tokens, new_labels):\n",
        "        ans+=\"{}\\t{}\".format(label, token)\n",
        "        ans+=\"\\n\"\n",
        "    nouns_from_sentence = re.findall(r'NN\\w*\\s+(\\w+)', ans)\n",
        "\n",
        "    return nouns_from_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXLQuu2DOBzo",
        "outputId": "6462d510-1256-4739-c499-7993a1992716"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['places', 'students', 'building']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example of using the function\n",
        "get_nouns(\"I think there should be many various sitting and studying places for students, both inside and outside of the building.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e38gRWFU2Sia"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text  # noqa\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "\n",
        "def get_c_similarity(sentence):\n",
        "    # Loading models from tfhub.dev\n",
        "    encoder = hub.KerasLayer(\"https://tfhub.dev/jeongukjae/smaller_LaBSE_15lang/1\")\n",
        "    preprocessor = hub.KerasLayer(\"https://tfhub.dev/jeongukjae/smaller_LaBSE_15lang_preprocess/1\")\n",
        "\n",
        "    # Constructing model to encode texts into high-dimensional vectors\n",
        "    sentences = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"sentences\")\n",
        "    encoder_inputs = preprocessor(sentences)\n",
        "    sentence_representation = encoder(encoder_inputs)[\"pooled_output\"]\n",
        "    normalized_sentence_representation = tf.nn.l2_normalize(sentence_representation, axis=-1)  # for cosine similarity\n",
        "    model = tf.keras.Model(sentences, normalized_sentence_representation)\n",
        "\n",
        "    # Encoding sentences.\n",
        "    CheckSentence = tf.constant([sentence])\n",
        "    Categories = tf.constant([\"Environment and climate resilience\", \"Mobility (transport)\", \"Local identity\", \"Future of work\", \"Land use\"])\n",
        "\n",
        "    sentence_embeds = model(CheckSentence)\n",
        "    categories_embeds = model(Categories)\n",
        "\n",
        "    # sentence-categories similarity to list\n",
        "    tensor_list = tf.tensordot(sentence_embeds, categories_embeds, axes=[[1], [1]]).numpy().tolist()\n",
        "    \n",
        "    return tensor_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfcza3S2Wgs7",
        "outputId": "a054a08b-d378-4995-bdd1-a79c9873f8af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[0.20263248682022095,\n",
              "  0.06531611829996109,\n",
              "  0.0811031311750412,\n",
              "  0.1793404370546341,\n",
              "  0.0731455385684967]]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example of using the function\n",
        "get_c_similarity(\"I think there should be many various sitting and studying places for students, both inside and outside of the building.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9N-aMj7KwK4"
      },
      "outputs": [],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RyVbMEpVKkj"
      },
      "outputs": [],
      "source": [
        "!pip install rollbar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrJ7t0JP9FzG"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import rollbar\n",
        "rollbar.init('3524a066b047491b9d777810d89dbfe4', 'testenv')\n",
        "# Set up the OpenAI API client\n",
        "openai.api_key = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KoHXTfpLB1F"
      },
      "outputs": [],
      "source": [
        "def ask_chatgpt(question):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model='gpt-3.5-turbo',\n",
        "        n=1,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a chatbot\"},\n",
        "            {\"role\": \"user\", \"content\": question},\n",
        "        ])\n",
        "\n",
        "    result = ''\n",
        "    for choice in response.choices:\n",
        "     result += choice.message.content\n",
        "     return (result)\n",
        "\n",
        "def gpt_ans(test_sentence, nouns, c_similarity_list):\n",
        " query = f\"\"\"for this sentance: {test_sentence} the nouns are: {nouns} and the cos similarity is: {c_similarity_list} Now I want you to tell me,\n",
        " given the nouns in the sentence and the cos similarity, for each of the five key areas, does the sentence fall. please write your answer in the following format:\n",
        " 1. Environment and climate resilience: Yes/No\n",
        " 2. Mobility (transport): Yes/No\n",
        " 3. local identity: Yes/No\n",
        " 4. future of work: Yes/No\n",
        " 5. land use: Yes/No\n",
        " if you cannot provide an answer for the five key areas, return 'No' for each key area with the format above.\n",
        " if there are no nouns in the sentence, still classify each of the five key areas, does the sentence fall with the format above\"\"\"\n",
        " try:\n",
        "     return ask_chatgpt(query)\n",
        " except Exception as e:\n",
        "     # monitor exception using Rollbar\n",
        "     rollbar.report_exc_info()\n",
        "     return e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Rhc_W20KLrD7",
        "outputId": "a9e8458b-513a-42f9-e634-09287e274edb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1. Environment and climate resilience: No\\n2. Mobility (transport): No\\n3. local identity: No\\n4. future of work: No\\n5. land use: No'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example of using the function\n",
        "sen = \"I think there should be many various sitting and studying places for students, both inside and outside of the building.\"\n",
        "nouns = get_nouns(sen)\n",
        "c_sim = get_c_similarity(sen)\n",
        "example = gpt_ans(sen, nouns, c_sim)\n",
        "example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Apmw4QHjkdfM",
        "outputId": "1ad21d54-cc3f-4188-d8be-69547adebef4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data length: 301\n",
            "Test data length: 76\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_data, test_data = train_test_split(client_message, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Train data length:\", len(train_data))\n",
        "print(\"Test data length:\", len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAww2x2al-Xx"
      },
      "outputs": [],
      "source": [
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fz16RBrHJTQe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def parse_and_append(output, sentence, df):\n",
        "    lines = output.strip().split('\\n')\n",
        "    classes = []\n",
        "\n",
        "    for line in lines:\n",
        "        # Only process lines starting with a number followed by a period\n",
        "        if len(line) >= 2 and line[0].isdigit() and line[1] == '.':\n",
        "            key_area, value = line[2:].split(':')\n",
        "            key_area = key_area.strip()\n",
        "            value = value.strip()\n",
        "\n",
        "            if value.lower() == 'yes':\n",
        "                classes.append(key_area)\n",
        "\n",
        "    if not classes:\n",
        "        classes.append('None')\n",
        "\n",
        "    new_rows = pd.DataFrame({\"sentence\": [sentence] * len(classes), \"class\": classes})\n",
        "    df = pd.concat([df, new_rows], ignore_index=True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "dKDcH9nUWgjo",
        "outputId": "17e615f4-0caf-41d4-e8e3-98b18f494b0f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a3e7b478-6216-4418-8612-e95e991a3494\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I think there should be many various sitting a...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a3e7b478-6216-4418-8612-e95e991a3494')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a3e7b478-6216-4418-8612-e95e991a3494 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a3e7b478-6216-4418-8612-e95e991a3494');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                            sentence class\n",
              "0  I think there should be many various sitting a...  None"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example of using the function\n",
        "example_df = pd.DataFrame(columns=[\"sentence\", \"class\"])\n",
        "parse_example = parse_and_append(example, sen, example_df)\n",
        "parse_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ne0M6jtwho1D"
      },
      "outputs": [],
      "source": [
        "# Create an empty DataFrame with the desired column names\n",
        "train_df = pd.DataFrame(columns=[\"sentence\", \"class\"])\n",
        "\n",
        "# Process each sentence in the list\n",
        "for sen in train_data:\n",
        "    nouns = get_nouns(sen)\n",
        "    c_similarity = get_c_similarity(sen)\n",
        "    output = gpt_ans(sen, nouns, c_similarity)\n",
        "    # print(output)\n",
        "    train_df = parse_and_append(output, sen, train_df)\n",
        "\n",
        "# Display the DataFrame\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jB5njAVeIsZ"
      },
      "outputs": [],
      "source": [
        "# Create an empty DataFrame with the desired column names\n",
        "test_df = pd.DataFrame(columns=[\"sentence\", \"class\"])\n",
        "\n",
        "# Process each sentence in the list\n",
        "for sen in test_data:\n",
        "    nouns = get_nouns(sen)\n",
        "    c_similarity = get_c_similarity(sen)\n",
        "    output = gpt_ans(sen, nouns, c_similarity)\n",
        "    # print(output)\n",
        "    test_df = parse_and_append(output, sen, train_df)\n",
        "\n",
        "# Display the DataFrame\n",
        "test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7OKzVzv_S0Q"
      },
      "source": [
        "Here we split the data to features and labels for the train and the test "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NadZ68pdl3ef"
      },
      "outputs": [],
      "source": [
        "# Create empty lists to hold the sentences and classes\n",
        "sentences_train = []\n",
        "classes_train = []\n",
        "\n",
        "# Iterate over the rows in the DataFrame\n",
        "for index, row in train_df.iterrows():\n",
        "    # Add the sentence and class to their respective lists\n",
        "    sentences_train.append(row[\"sentence\"])\n",
        "    classes_train.append(row[\"class\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrzkFjvzm4m9"
      },
      "outputs": [],
      "source": [
        "# Create empty lists to hold the sentences and classes\n",
        "sentences_test = []\n",
        "classes_test = []\n",
        "\n",
        "# Iterate over the rows in the DataFrame\n",
        "for index, row in test_df.iterrows():\n",
        "    # Add the sentence and class to their respective lists\n",
        "    sentences_test.append(row[\"sentence\"])\n",
        "    classes_test.append(row[\"class\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjePJAr4-9CL"
      },
      "source": [
        "#Build the model\n",
        "In here we train the model where the labels are based on the chatGPT results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0KX7vVGnW8g"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create TF-IDF embeddings for the sentences\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train = vectorizer.fit_transform(sentences_train)\n",
        "X_test = vectorizer.transform(sentences_test)\n",
        "\n",
        "# Encode the class labels\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(classes_train)\n",
        "y_test = le.transform(classes_test)\n",
        "\n",
        "# Train the Adaboost classifier on the training data\n",
        "clf = AdaBoostClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Use the trained classifier to predict the classes for the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}